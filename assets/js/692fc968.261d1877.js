"use strict";(self.webpackChunkopm_documentation=self.webpackChunkopm_documentation||[]).push([[5897],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>u});var a=n(7294);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,s=function(e,t){if(null==e)return{};var n,a,s={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var l=a.createContext({}),m=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=m(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,s=e.mdxType,i=e.originalType,l=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),d=m(n),h=s,u=d["".concat(l,".").concat(h)]||d[h]||p[h]||i;return n?a.createElement(u,o(o({ref:t},c),{},{components:n})):a.createElement(u,o({ref:t},c))}));function u(e,t){var n=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var i=n.length,o=new Array(i);o[0]=h;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[d]="string"==typeof e?e:s,o[1]=r;for(var m=2;m<i;m++)o[m]=n[m];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},1665:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>m});var a=n(7462),s=(n(7294),n(3905));const i={sidebar_label:"The Recommendation System (v1.0)",sidebar_position:81},o="Recommendation Systems in Media Publishing",r={unversionedId:"recommendation-system/index",id:"recommendation-system/index",title:"Recommendation Systems in Media Publishing",description:"Project Developer & Writer: Kevin Jang",source:"@site/docs/recommendation-system/index.md",sourceDirName:"recommendation-system",slug:"/recommendation-system/",permalink:"/docs/recommendation-system/",draft:!1,editUrl:"https://github.com/Other-People-UCSD/docs/tree/main/docs/recommendation-system/index.md",tags:[],version:"current",sidebarPosition:81,frontMatter:{sidebar_label:"The Recommendation System (v1.0)",sidebar_position:81},sidebar:"docsSidebar",previous:{title:"Data Science Using the Post API",permalink:"/docs/data-science"},next:{title:"Recommendation Workflow",permalink:"/docs/recommendation-workflow"}},l={},m=[{value:"Background",id:"background",level:3},{value:"Metadata Count Feature Recommender",id:"metadata-count-feature-recommender",level:2},{value:"Content-Only TF-IDF Recommender",id:"content-only-tf-idf-recommender",level:2},{value:"Meta + Content TF-IDF Recommender",id:"meta--content-tf-idf-recommender",level:2},{value:"Artwork Visual Similarity Recommendation",id:"artwork-visual-similarity-recommendation",level:2},{value:"Loading and Formatting Image Data",id:"loading-and-formatting-image-data",level:3},{value:"Using the pretrained VGG16 on imagenet",id:"using-the-pretrained-vgg16-on-imagenet",level:3},{value:"Define the Recommendation System",id:"define-the-recommendation-system",level:3},{value:"Fit the Image Recommender to Predict Posts",id:"fit-the-image-recommender-to-predict-posts",level:3},{value:"Combining the Artwork Model with Literature Model",id:"combining-the-artwork-model-with-literature-model",level:2},{value:"Combination 1: Visual Features and (Meta-TF-IDF)",id:"combination-1-visual-features-and-meta-tf-idf",level:3},{value:"Combination 2: Normalized Visual Features and (Meta-TF-IDF)",id:"combination-2-normalized-visual-features-and-meta-tf-idf",level:3},{value:"Combination 3: Mean Cosine Visual and (Meta-TF-IDF)",id:"combination-3-mean-cosine-visual-and-meta-tf-idf",level:3},{value:"Combination 4: Mean Cosine Visual + TF-IDF + Metadata Count",id:"combination-4-mean-cosine-visual--tf-idf--metadata-count",level:3},{value:"Closing Remarks",id:"closing-remarks",level:2}],c={toc:m},d="wrapper";function p(e){let{components:t,...i}=e;return(0,s.kt)(d,(0,a.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"recommendation-systems-in-media-publishing"},"Recommendation Systems in Media Publishing"),(0,s.kt)("p",null,"Project Developer & Writer: Kevin Jang"),(0,s.kt)("h3",{id:"background"},"Background"),(0,s.kt)("p",null,"The Other People Magazine website hosts hundreds of posts and artworks with a search bar to filter out posts that a user is looking for. When the user has not made a query but focuses on the search bar, a set of the most recent ten posts published are displayed. "),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},"What if the user enjoyed reading a literary piece and wanted similar stories?")),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},"What if the user wanted to see similar artworks?")),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},"What if the user wanted to find other posts with similar tags or even languages?")),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Product recommendation systems")," attempt to resolve these user cases by recommending similar works to the current work. These systems also act as filters by predicting the next product to recommend based on the inputs fed through the system. These systems work by taking in a set of inputs, extracting features from each input, defining a similarity function, and finally taking the K nearest neighbors."),(0,s.kt)("p",null,"Other People Magazine's use case differs by the ",(0,s.kt)("em",{parentName:"p"},"products")," that it is able to recommend. We are dealing with ",(0,s.kt)("strong",{parentName:"p"},"literature")," and ",(0,s.kt)("strong",{parentName:"p"},"art"),", two highly subjected fields with which to predict the user's expectations. Our organization has a limitation in which we are unable to build recommendations between users or store the state of page breadcrumbs that the user created. This limitation comes from the fact that the website is client-based and not hosted on its own server where this could be accomplished through the use of cookies. Despite these limitations, we can still recommend creative works using ",(0,s.kt)("strong",{parentName:"p"},"content-based recommendation systems"),". This Notebook will go through the different kinds of models and test their predictions to determine the best approach to recommending subjectivity."),(0,s.kt)("h2",{id:"metadata-count-feature-recommender"},"Metadata Count Feature Recommender"),(0,s.kt)("p",null,"This recommendation model is meant to extract features from the metadata in each post and determine similarity between them. Of the metadata features, four influence the recommendation the most: ",(0,s.kt)("inlineCode",{parentName:"p"},"tags"),", ",(0,s.kt)("inlineCode",{parentName:"p"},"contributor"),", ",(0,s.kt)("inlineCode",{parentName:"p"},"wordCount"),", and ",(0,s.kt)("inlineCode",{parentName:"p"},"title"),'. The tags would be a good recommendation for readers who are interested in works of the same genre. The contributors, also known as the writers and/or artists, could interest the reader and thus recommend similar works by the authors. Word count can also play a role in determining if the reader wants to enjoy works of a similar length. In specific cases, the title could have an influence on the prediction. For example, "Mother Moon", a visual artwork should recommend "Mother Moon (interview)" and vice versa because they have high relevance to each other.'),(0,s.kt)("p",null,"This model will use a count vector of the terms that show up in the metadata as a simple demonstration of how the recommender systems work. It is not expected to be a highly accurate model and the reasons will be explained at the end of this section."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import requests\nimport json\nimport re\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n")),(0,s.kt)("p",null,"To access the creative works at Other People, we can utilize the API built in the frontend to extract particular information from each post."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"res = requests.get('https://www.otherpeoplesd.com/api/post-data.json')\ndata = json.loads(res.text)\npostSlugs = data.keys()\n# Convert the json data to a list for indexing\nlistRes = [k for k,v in data.items()]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Converts array metadata by converting each element into a single word\n# Removes additional styling details in how contributor or tag fields were written\ndef reformatArray(arr, key):\n    try:\n        if key == 'contributor':\n            arr = re.split(',|&', arr)\n            \n        reformatted = []\n        for item in arr:\n            if key == 'contributor':\n                item = re.sub(r'\\(.*\\)', '', item)\n                \n            item = item.strip()\n            words = item.split(' ')\n            \n            if key == 'tags' and len(words) > 1:\n                for w in words:\n                    reformatted.append(w)\n                    \n            item = ''.join(words)\n            reformatted.append(item)\n        return reformatted\n    except:\n        return arr\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"metaFeats = []\nfor post in postSlugs:\n    postData = data[post]\n        \n    postData['contributor'] = reformatArray(postData['contributor'], 'contributor')\n    postData['tags'] = reformatArray(postData['tags'], 'tags')\n    \n    # Force edit word count to the manually set word count on custom-built works\n    if 'manualWC' in postData:\n        postData['wordCount'] = postData['manualWC']\n        del postData['manualWC']\n    \n    feat = ''.join(postData['title']) + ' ' + ' '.join(postData['contributor']) + ' ' + ' '.join(postData['tags']) + ' ' + str(postData['wordCount'])\n    feat = feat.lower()\n    metaFeats.append(feat)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Example of reformatted metadata features\nmetaFeats[0]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"'nesting dolls and snakeskin ivydu fiction 2926'\n")),(0,s.kt)("p",null,"After formatting the metadata for each post, features must be extracted out of them in a way that they can be compared against each other properly. This is done by taking a count vector to determine how often terms appear in the features. "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"count = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(metaFeats)\nprint(count_matrix.shape)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 532)\n")),(0,s.kt)("p",null,"Now that the features have been extracted, we must define a similarity function. "),(0,s.kt)("p",null,"The two most common similarity functions for recommendation systems are ",(0,s.kt)("strong",{parentName:"p"},"Euclidean distance")," and ",(0,s.kt)("strong",{parentName:"p"},"cosine similarity"),". Euclidean distance measures the distance between two points while cosine similarity measures the angle between the two points. Euclidean distance is affected by magnitude, so it isn't a great option when taking a frequency count for the extracted features. The angle would be better because it won't be affected too much by magnitude changes."),(0,s.kt)("p",null,"$$\\text{Euclidean Distance}$$\n$$\\quad d(p,q) = \\sqrt{\\sum^n_{i=1}(p_i-q_i)^2}$$"),(0,s.kt)("p",null,"$$\\text{Cosine Similarity}$$\n$$cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} = \\frac{\\sum^n",(0,s.kt)("em",{parentName:"p"},"{i=1}A_i \\cdot B_i}{\\sqrt{\\sum^n"),"{i=1}A",(0,s.kt)("em",{parentName:"p"},"i^2 \\cdot \\sum^n"),"{i=1}B_i^2}}$$"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_count = cosine_similarity(count_matrix, count_matrix)\nprint(cosine_sim_count.shape)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 148)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Recommendation model with k closest works (K closest neighbors algorithm)\ndef recommender_system(slug, cosine_sim,\n                       k=10, showSim=False, sigFigs=4):\n    idx = listRes.index(slug)\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:k+1]\n    if showSim:\n        sim_slugs = [(listRes[item[0]], round(item[1], sigFigs)) for item in sim_scores] \n    else:\n        sim_slugs = [listRes[item[0]] for item in sim_scores]\n    return sim_slugs\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"rec_metadata = recommender_system('/3/blame-and-balm', cosine_sim_count,\n                             k=10, showSim=True, sigFigs=4)\nrec_metadata\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('/3/blame-and-balm-interview', 0.4629),\n ('/2023/out-of-use', 0.2041),\n ('/2023/planet', 0.2041),\n ('/2023/too-easily', 0.2041),\n ('/4/kalbelia', 0.2041),\n ('/2022/dictionary', 0.2041),\n ('/2022/routines', 0.2041),\n ('/2021/butterflies', 0.2041),\n ('/2021/puppydom', 0.2041),\n ('/1/2230', 0.2041)]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"rec_metadata = recommender_system('/1/untitled', cosine_sim_count,\n                             k=10, showSim=True, sigFigs=4)\nrec_metadata\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('/2/underdog', 0.6),\n ('/2/now-playing', 0.6),\n ('/1/fronteras', 0.6),\n ('/1/monotony', 0.6),\n ('/1/skin-esbensen', 0.6),\n ('/1/dissonance', 0.6),\n ('/4/in-a-haze', 0.5477),\n ('/4/teacup', 0.5477),\n ('/4/highway', 0.5477),\n ('/3/abiding-vestige', 0.5477)]\n")),(0,s.kt)("p",null,"As seen in both of these results, many recommendations have the same similarity score for both prose and visual arts pages. This is an issue because the low dimensionality aided by many works with the same tags makes it harder to tell if the recommended work is truly appropriate. Also, the posts in the API are sorted in chronological order according to when they were posted, so the most recent works are recommended first. This disadvantages older posts from being visible in the recommendation. We can check how much loss occurs by looking at the connected components made in the recommendation graph and seeing what posts barely get recommended."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"metaRecs = {}\nfor slug in listRes:\n    rec_metadata = recommender_system(slug, cosine_sim_count,\n                                         k=10, showSim=False)\n    metaRecs[slug] = rec_metadata\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Print connected components in the recommendation graph \n# to check the diversity of recommendation\n\ndef test_rec_diversity(recommendations, showConnections=False):\n    def dfs(u): \n        if u in seenSet:\n            return\n        seenSet.add(u)\n        for v in recommendations[u]:\n            dfs(v)\n    # Find direct recommendations\n    countDirects = {}\n    for post in postSlugs:\n        seenSet = set()\n        dfs(post)\n        if showConnections:\n            print(len(seenSet), post)\n        countDirects[post] = 0\n        for direct in recommendations[post]:\n            countDirects[direct] = countDirects.get(direct, 0) + 1\n    sortedDirect = sorted(countDirects.items(), key=lambda i: i[1])\n    return sortedDirect\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"meta_direct_recs = test_rec_diversity(metaRecs, showConnections=False)\nmeta_direct_counts = {}\nfor k,v in meta_direct_recs:\n    meta_direct_counts[v] = meta_direct_counts.get(v, 0) + 1\nprint('(k, num) posts with direct recommendations:', meta_direct_counts)\n# sortedDirect\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(k, num) posts with direct recommendations: {0: 50, 1: 22, 2: 22, 3: 13, 4: 3, 5: 7, 6: 2, 7: 4, 8: 1, 10: 1, 11: 2, 12: 1, 15: 2, 16: 1, 17: 2, 18: 2, 19: 2, 20: 1, 21: 1, 22: 1, 31: 1, 32: 1, 38: 1, 40: 1, 45: 1, 47: 1, 50: 1, 52: 1}\n")),(0,s.kt)("p",null,"There are fifty posts that do not get recommended by any other post. This is a significant issue because a third of all webpages do not get recommended at all! Even worse, two-thirds of all webpages get recommended at most two times. This is a bad recommendation model that requires more input information to make more diverse recommendations."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Convert the result to JSON and write to file\nwith open('recommender-meta.json', mode='w', encoding='utf8') as outfile:\n    outfile.write(json.dumps(metaRecs))\n")),(0,s.kt)("h2",{id:"content-only-tf-idf-recommender"},"Content-Only TF-IDF Recommender"),(0,s.kt)("p",null,"This model makes recommendations based on the content of works through the ",(0,s.kt)("inlineCode",{parentName:"p"},"noHTML")," key. Because there will be many different words from the content and styles of writers, the corpus accumulated by taking a simple count vector will make many frequencies zero in the similarity matrix. There is also an issue of semantic context that the count vector will not address. To address these issue, we will be using TF-IDF to extract and vectorize the features so that certain terms will be weighed higher than others. "),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},"Term frequency")," (TF) is used to measure the frequency that a term has occurred in the data. ",(0,s.kt)("em",{parentName:"p"},"Inverse document frequency")," (IDF) measures how important a term is in the data by checking its rarity across all works. These two factors are compared in cosine similarity to recommend works that have similar important keywords or topics to match what a reader may like to consume next."),(0,s.kt)("p",null,"$$tf(t,d) = \\frac{\\text{frequency}}{\\text{frequency of most common word}}$$\n$$idf(t,D) = -log P(t|D) = log \\frac{N}{|","{","d \\in D: t \\in d","}","|}$$"),(0,s.kt)("p",null,"$$tfidf(t,d,D) = tf(t,d) \\times idf(t,D)$$"),(0,s.kt)("p",null,"While TF-IDF would work on only metadata, it would not have been a strong model because of a small corpus to make recommendations on. That is why a simple, lightweight count sufficed for checking similarity before taking the cosine."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Do NOT add alt text to the content!\n# The alt text was not written by the author or artist.\n# Image similarity resolves this by examining the features.\nonlyContent = []\nfor slug in postSlugs:\n    post = data[slug]\n    altIdx = post['noHTML'].find('img_alt:')\n    if altIdx != -1:\n        onlyContent.append(post['noHTML'][:altIdx])\n        # img_alt = post['noHTML'][altIdx:]\n        # img_alt = re.sub('img_alt: ', '', img_alt)\n        # newList = []\n        # for w in img_alt.split(' '):\n        #     newList += ['img_alt_' + w]\n        # onlyContent.append(post['noHTML'][:altIdx] + ' '.join(newList))\n    else:\n        onlyContent.append(post['noHTML'])\n")),(0,s.kt)("p",null,"To address semantic context inside the posts, we can choose the n-grams parameter to pass into the TF-IDF vector. N-grams means the number of adjacent words that form a term in the corpus. Bigrams provides more accurate recommendations in this model because viewing adjacent words can identify positive or negative connotations happening in the story. "),(0,s.kt)("p",null,"This model uses both unigrams and bigrams for the feature vector. This was found to have the most accurate predictions."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"ngram_range = (1,2)\ntfidf = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\ntfidf_matrix = tfidf.fit_transform(onlyContent)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"tfidf_matrix.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 54400)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_tfidf = cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_sim_tfidf.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 148)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"rec_tfidf = recommender_system('/3/blame-and-balm', cosine_sim_tfidf,\n                             k=10, showSim=True, sigFigs=4) \nrec_tfidf\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('/2023/stillwater-saint', 0.0428),\n ('/2023/nesting-dolls-and-snakeskin', 0.0318),\n ('/3/high-tide', 0.0286),\n ('/2/fishbowl-brain', 0.0286),\n ('/2023/dear-guma', 0.0272),\n ('/3/the-middle-of-all-middles', 0.0255),\n ('/2023/the-greatest-author', 0.0254),\n ('/3/sing', 0.0249),\n ('/2022/validation', 0.0241),\n ('/2021/requiem', 0.024)]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"rec_tfidf = recommender_system('/5/ascension', cosine_sim_tfidf,\n                             k=10, showSim=True, sigFigs=4) \nrec_tfidf\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('/2023/hole', 0.0),\n ('/2023/out-of-use', 0.0),\n ('/2023/planet', 0.0),\n ('/2023/outdated', 0.0),\n ('/2023/the-greatest-author', 0.0),\n ('/2023/too-easily', 0.0),\n ('/6/you-have-created-an-imaginary-friend', 0.0),\n ('/6/editors-note', 0.0),\n ('/2023/stillwater-saint', 0.0),\n ('/2023/i-keep-driving-east', 0.0)]\n")),(0,s.kt)("p",null,'There is much more variation in recommendations with this content model, although we lose the strong recommendation of the interview piece that should be paired with "Blame and Balm". If the metadata-based and content-based recommenders were combined, the recommendation could become stronger.'),(0,s.kt)("p",null,"Note that if we attempt to view recommendation for visual art pieces, the similarities are all zero. These pages have no content","\u2014","no words to extract features for. This leaves visual arts pieces worse off than the simple metadata recommender."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"tfidfRecs = {}\nfor slug in listRes:\n    rec_tfidf = recommender_system(slug, cosine_sim_tfidf,\n                                   k=10, showSim=False)\n    tfidfRecs[slug] = rec_tfidf\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"tfidf_direct_recs = test_rec_diversity(tfidfRecs, showConnections=False)\ntfidf_direct_counts = {}\nfor k,v in tfidf_direct_recs:\n    tfidf_direct_counts[v] = tfidf_direct_counts.get(v, 0) + 1\nprint('(k, num) posts with direct recommendations:', tfidf_direct_counts)\ntfidf_direct_recs[-10:]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(k, num) posts with direct recommendations: {0: 65, 1: 27, 2: 9, 3: 9, 4: 4, 5: 4, 6: 2, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 16: 1, 18: 1, 20: 1, 23: 3, 26: 1, 30: 1, 31: 1, 32: 1, 33: 2, 35: 1, 36: 1, 39: 2, 45: 2, 46: 1, 55: 1, 60: 1, 67: 1}\n\n\n\n\n\n[('/2023/the-greatest-author', 35),\n ('/6/editors-note', 36),\n ('/2023/hole', 39),\n ('/2023/always-the-victim', 39),\n ('/2023/nesting-dolls-and-snakeskin', 45),\n ('/4/mirror-shards', 45),\n ('/5/lament-for-the-old-man', 46),\n ('/2023/stillwater-saint', 55),\n ('/2023/outdated', 60),\n ('/2023/homewrecker', 67)]\n")),(0,s.kt)("p",null,'Around two thirds of all posts have no more than one recommend post. However, we can see the relationship between the recommendations with zero similarity by comparing the posts recommended in chronological order to the posts that have the most direct recommendations. "Hole", "Outdated", "Always the Victim" are all in the top ten.'),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Convert the result to JSON and write to file\nwith open('recommender-content.json', mode='w', encoding='utf8') as outfile:\n    outfile.write(json.dumps(tfidfRecs))\n")),(0,s.kt)("h2",{id:"meta--content-tf-idf-recommender"},"Meta + Content TF-IDF Recommender"),(0,s.kt)("p",null,"This model combines the metadata feature from the metadata and the content string into a single feature string. TF-IDF is used to build the recommendations. To make the metadata have higher signifcance than the content of the posts and to prevent terms from being counted with the content of the post, each metadata feature should be labeled separately. We will attach a string prefix to each of the formatted metadata features "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Tag the strings in metadata to be distinct from the textual content\nfor i, meta in enumerate(metaFeats):\n    newList = []\n    for j, w in enumerate(meta.split(' ')):\n        newList += ['meta_' + w]\n    metaFeats[i] = ' '.join(newList)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"meta_content_feat = np.array([l1 + ' ' + l2 for l1, l2 in zip(metaFeats, onlyContent)])\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"ngram_range = (1,3)\ntfidf = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\nmeta_tfidf_matrix = tfidf.fit_transform(meta_content_feat)\nprint(meta_tfidf_matrix.shape)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 103094)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_meta_tfidf = cosine_similarity(meta_tfidf_matrix, meta_tfidf_matrix)\nprint(cosine_sim_meta_tfidf.shape)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 148)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"rec_tfidf = recommender_system('/3/blame-and-balm', cosine_sim_meta_tfidf,\n                               k=10, showSim=True, sigFigs=4)\nrec_tfidf\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('/2023/stillwater-saint', 0.0286),\n ('/2023/nesting-dolls-and-snakeskin', 0.0207),\n ('/3/blame-and-balm-interview', 0.0195),\n ('/2/fishbowl-brain', 0.0189),\n ('/3/high-tide', 0.0179),\n ('/2023/the-greatest-author', 0.0172),\n ('/3/the-middle-of-all-middles', 0.0169),\n ('/2023/dear-guma', 0.0166),\n ('/3/sing', 0.0163),\n ('/2020/hope-came-last', 0.0146)]\n")),(0,s.kt)("p",null,'This result shows that the interview is included in the recommendations! However, the interview is not the top result. This could mean several things such as the content in "Stillwater Saint" is a better match wheras the interview is less relevant to the actual "Blame and Balm" story.'),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"meta_tfidf_Recs = {}\nfor slug in listRes:\n    rec_meta_tfidf = recommender_system(slug, cosine_sim_meta_tfidf,\n                                         k=10, showSim=False)\n    meta_tfidf_Recs[slug] = rec_meta_tfidf\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"meta_tfidf_direct_recs = test_rec_diversity(meta_tfidf_Recs, showConnections=False)\nmeta_tfidf_direct_counts = {}\nfor k,v in meta_tfidf_direct_recs:\n    meta_tfidf_direct_counts[v] = meta_tfidf_direct_counts.get(v, 0) + 1\nprint('(k, num) posts with direct recommendations:', meta_tfidf_direct_counts)\nmeta_tfidf_direct_recs[-10:]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(k, num) posts with direct recommendations: {0: 44, 1: 32, 2: 14, 3: 15, 4: 8, 5: 3, 6: 4, 7: 2, 8: 2, 9: 1, 10: 4, 11: 3, 12: 1, 16: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 3, 27: 1, 34: 1, 37: 1, 39: 1, 42: 1, 49: 1, 65: 1}\n\n\n\n\n\n[('/2023/dear-guma', 22),\n ('/3/sing', 22),\n ('/4/worm', 22),\n ('/2023/stillwater-saint', 27),\n ('/2023/always-the-victim', 34),\n ('/2023/outdated', 37),\n ('/2023/nesting-dolls-and-snakeskin', 39),\n ('/4/mirror-shards', 42),\n ('/5/lament-for-the-old-man', 49),\n ('/2023/homewrecker', 65)]\n")),(0,s.kt)("p",null,"Less than a third of all posts do not have a recommendation, but now two thirds of all posts have at most four recommendations. This model has taken the shortcomings of the metadata and content-only models to diversify the result more and also have more accurate predictions between posts relevant to each other!"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Convert the result to JSON and write to file\nwith open('recommender-both.json', mode='w', encoding='utf8') as outfile:\n    outfile.write(json.dumps(meta_tfidf_Recs))\n")),(0,s.kt)("p",null,"It is too early to conclude this as a successful model when we still have a major persistent issue. Let's look at an example of a work that is not text-based, e.g. an artwork."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"recommender_system('/1/treasure', cosine_sim_meta_tfidf, \n                k=10, showSim=True, sigFigs=6)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('/1/skin-vilar', 0.03301),\n ('/1/skin-esbensen', 0.031624),\n ('/1/dissonance', 0.031321),\n ('/1/fronteras', 0.03074),\n ('/2/underdog', 0.030462),\n ('/1/untitled', 0.030462),\n ('/1/monotony', 0.030462),\n ('/4/teacup', 0.027562),\n ('/4/highway', 0.027562),\n ('/2/life-cycle', 0.027006)]\n")),(0,s.kt)("p",null,"There are several works that still have the same similarity values. The reason the recommendations are nonzero is because of the metadata features that are similar to the artwork while the TF-IDF content part would still remain as zero. We can do better with our literary arts recommendation system by now turning our focus to visual similarity recommendation models."),(0,s.kt)("h2",{id:"artwork-visual-similarity-recommendation"},"Artwork Visual Similarity Recommendation"),(0,s.kt)("p",null,"While literature can be modeled through content-based recommendation systems, art must take a different approach. This is because 1. images are a matrix of pixels ranging from 0 to 255 and 2. art is subjective, thus the recommendations may not match the user's preference without much needed user input. Without tracking breadcrumbs to figure out what the user's interests are, we can instead build a recommendation system independent of user input via visual similarity modeling."),(0,s.kt)("p",null,"This recommendation system uses deep learning to perform image recommendation because of how image data must be converted into features for comparison. There are many different ways to compare image data, but we will be using the convolutional neural network (CNN) VGG16 model with transfer learning for image recommendation. VGG16 is commonly used in image classification and it can be pretrained with a huge set of images to figure out how to classify these images based on many convolution and pooling layers. Because the model is being trained on imagenet, a database of over a million images, to best-fit its hyperparameters, we can input our own small set of data to make accurate predictions using its hyperparameters through the concept of transfer learning."),(0,s.kt)("p",null,"However, the concepts of image classification and image similarity differ and this influences how VGG16 is used in the system. Image classification attempts to group images into clusters or groups to make recommendations, but it entraps the inputs because it can only classify based on the kinds of labels the input can be. Image similarity does not use labels to fix an input into a defined category and instead uses the difference between features in the images to determine the output. To make VGG16 work for image similarity, the output layer must be changed such that it extracts features instead of outputting predictions based on the data labels."),(0,s.kt)("p",null,"To find the difference between images, we use cosine similarity over the features in each image. The CNN finds the best representation for the images and thus these features represent how the image is compared to others. As such, the angle between images would be a more accurate determiner of similarity rather than taking the euclidean distances. For example, two identical images may have changed only by their BRG/RBG color magnitude and thus they may have a high distance between each other, but their angle should be around the same because of the color edges and textures in the image. After creating the cosine similarity matrix, we can repurpose the same recommendation algorithm used in the content-based recommendation system to get our recommendations through K closest neighbors."),(0,s.kt)("p",null,"Using the cosine also gives the benefit of combining the visual features to the text content features such that recommendations on the works with both literature and art can lead to recommendations that branch into both literature and visual arts. This is a significant benefit because the artworks found in a work of literature could help recommend posts with the same atmosphere based on similar images. Visual arts pages will also find themselves in places of more engagement as they are being recommended on works of literature. "),(0,s.kt)("p",null,"To improve this recommender system, try using different models such as ResNet, VGG, AlexNet, or even custom CNNs trained on a large image set."),(0,s.kt)("h3",{id:"loading-and-formatting-image-data"},"Loading and Formatting Image Data"),(0,s.kt)("p",null,"Exposing artwork onto an API without proper authentication is not secure because adversaries could circumvent levels of copy-prevention by querying the API to copy the works. Therefore, the works should be downloaded to the machine so that they can be directly accessed through the local filesystem."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import requests\nimport json\nfrom tqdm.notebook import tnrange, tqdm\n# import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom os import walk\nimport cv2\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Get all posts regardless if they contain images\n# This endpoint runs a little faster than the post-data endpoint and we only need to get the postSlugs \nres = requests.get('https://www.otherpeoplesd.com/api/post-metadata.json')\ndata = json.loads(res.text)\npostSlugs = data.keys()\n# Convert the json data to a list for indexing\nlistRes = [k for k,v in data.items()]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Assuming the notebook is in the documentation repo (./Docs/) and the images are in the Calla-Lily repo\nimgDir = '../Calla-Lily/public/images/'\nimgPaths = []\nfor (dirpath, _, filenames) in walk(imgDir):\n    filePath = [dirpath.replace('\\\\', '/') + '/' + filename for filename in filenames]\n    imgPaths.extend(filePath)\nimgPaths[:5]\n\n# Display the images and their indices\n# for i,p in enumerate(imgPaths):\n#     print(i,p)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"['../Calla-Lily/public/images/1/amsterdam-nightlife-1-marcin-kierebinski.jpg',\n '../Calla-Lily/public/images/1/amsterdam-nightlife-2-marcin-kierebinski.jpg',\n '../Calla-Lily/public/images/1/boys-night-1-jack-yang.png',\n '../Calla-Lily/public/images/1/boys-night-2-jack-yang.png',\n '../Calla-Lily/public/images/1/brain-fish-zhilin-li.png']\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"postImgs = {}\n\nfor slug in tqdm(postSlugs, desc='assigning images to posts', position=0, leave=True):\n    postRes = requests.get('https://www.otherpeoplesd.com/api/post' + slug)\n    postData = json.loads(postRes.text)\n    postImgs[slug] = []\n    for fullPath in imgPaths:\n        path = fullPath.replace(imgDir, '')\n        if path in postData['contentHtml']:\n            postImgs[slug].append(fullPath)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"assigning images to posts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:45<00:00,  3.29it/s]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"def getPostFromImg(slug):\n    for k,imgPathArr in postImgs.items():\n        if slug in imgPathArr:\n            return k\n\ngetPostFromImg(imgDir + '1/amsterdam-nightlife-1-marcin-kierebinski.jpg')    \n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"'/1/i-left-my-heart-in-amsterdam'\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"def printImg(imgPath):\n    BGRimg = cv2.imread(imgPath)\n    img = cv2.cvtColor(BGRimg, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.show()\n")),(0,s.kt)("h3",{id:"using-the-pretrained-vgg16-on-imagenet"},"Using the pretrained VGG16 on imagenet"),(0,s.kt)("p",null,"Tensorflow has several ",(0,s.kt)("inlineCode",{parentName:"p"},"preprocess_input")," methods depending on the prebuilt model being used. We have found that using the imagenet ",(0,s.kt)("inlineCode",{parentName:"p"},"preprocess_input")," has better accuracy than VGG16's ",(0,s.kt)("inlineCode",{parentName:"p"},"preprocess_input"),". This could be because the inputs to the VGG16 model is imagenet data and thus it makes sense to transform our own image inputs to match imagenet. "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.models import Model\n# from tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.utils import load_img, img_to_array \nfrom sklearn.metrics.pairwise import cosine_similarity\n")),(0,s.kt)("p",null,"While the Keras documentation has examples of how to extract features instead of using the model for image classification (",(0,s.kt)("a",{parentName:"p",href:"https://keras.io/api/applications/#usage-examples-for-image-classification-models"},"https://keras.io/api/applications/#usage-examples-for-image-classification-models"),"), the prediction function throws errors at the input."),(0,s.kt)("p",null,"Changing the output layer to be the one before the prediction layer results in features extracted to a vector of size 4096."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"model_vgg16 = VGG16(weights='imagenet', include_top=True)\nfeature_extraction = Model(inputs=model_vgg16.input, outputs=model_vgg16.get_layer(\"fc2\").output)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"feature_extraction.summary()\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'Model: "model"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n flatten (Flatten)           (None, 25088)             0         \n                                                                 \n fc1 (Dense)                 (None, 4096)              102764544 \n                                                                 \n fc2 (Dense)                 (None, 4096)              16781312  \n                                                                 \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\n')),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"def readData_tf(imgPaths, target_size):\n    imgSlugs = []\n    imgData = []\n    for path in tqdm(imgPaths, desc='reading img data'):\n        # print(path)\n        if '.gif' in path:\n            continue\n        img = load_img(path, target_size=target_size, keep_aspect_ratio=True)\n        img = img_to_array(img)\n        # print(img.shape)\n        imgSlugs.append(path)\n        imgData.append(img)\n    return imgSlugs, imgData\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# target_size = minDim\ntarget_size = (224,224)\ntf_imgSlugs, tf_imgData = readData_tf(imgPaths, target_size=target_size)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"reading img data:  36%|\u2588\u2588\u2588\u258c      | 47/132 [00:03<00:04, 20.45it/s]C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (99000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\nreading img data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 132/132 [00:09<00:00, 13.48it/s]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"tf_imgData = np.array(tf_imgData)\nprint(tf_imgData.shape)\ntf_imgData = preprocess_input(tf_imgData)\nfeatures_tf = feature_extraction.predict(tf_imgData)\nprint(features_tf.shape)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(130, 224, 224, 3)\n5/5 [==============================] - 12s 2s/step\n(130, 4096)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_tf = cosine_similarity(features_tf)\ncosine_sim_tf.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(130, 130)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_tf\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"array([[1.0000001 , 0.5031772 , 0.23751105, ..., 0.1317079 , 0.28118014,\n        0.21835047],\n       [0.5031772 , 1.0000001 , 0.21519426, ..., 0.16504104, 0.28393784,\n        0.23923588],\n       [0.23751105, 0.21519426, 1.0000001 , ..., 0.19529504, 0.3781404 ,\n        0.24039938],\n       ...,\n       [0.1317079 , 0.16504104, 0.19529504, ..., 0.9999998 , 0.26677018,\n        0.24800706],\n       [0.28118014, 0.28393784, 0.3781404 , ..., 0.26677018, 0.9999998 ,\n        0.3685978 ],\n       [0.21835047, 0.23923588, 0.24039938, ..., 0.24800706, 0.3685978 ,\n        1.0000002 ]], dtype=float32)\n")),(0,s.kt)("h3",{id:"define-the-recommendation-system"},"Define the Recommendation System"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Retrieve the K closest images\ndef getKClosest(slug, k, showSim, sigFigs, sim_scores, parent_post, corrected=True):\n    res = []\n    i = 0\n    for item in sim_scores:\n        if '.gif' in imgPaths[item[0]]:\n            continue\n        if corrected and i < k:\n            if imgPaths[item[0]] in postImgs[parent_post]:\n                continue\n        if i == k:\n            return res\n        if showSim:\n            res.append((imgPaths[item[0]], round(item[1], sigFigs)))\n        else:\n            res.append(imgPaths[item[0]])\n        i += 1\n    return res\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Recommendation model with k closest works\ndef vgg16Recommender(slug, cosine_sim, k=10, showSim=False, sigFigs=4):\n    idx = imgPaths.index(slug)\n    parent_post = getPostFromImg(slug)\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:]\n    i = 0\n\n    corrected_sim_slugs = getKClosest(slug,k,showSim,sigFigs,\n                                      sim_scores,parent_post)\n    \n    uncorrected_sim_slugs = getKClosest(slug,k,showSim,sigFigs,\n                                        sim_scores,parent_post,corrected=False)\n    \n    return corrected_sim_slugs, uncorrected_sim_slugs\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"def displayRecs(inputImg, cosine_sim, corrected=True):\n    k = 10\n    recommend, uncorrected = vgg16Recommender(inputImg, cosine_sim=cosine_sim, \n                                              k=k, showSim=True, sigFigs=4)\n    print('original img')\n    printImg(inputImg)\n    \n    print('recommendations',('(corrected)' if corrected else '(uncorrected)'))\n    for i in range(k):\n        plt.subplot(5,2, i+1)\n        BGRimg = None            \n        if corrected:\n            BGRimg = cv2.imread(recommend[i][0])\n        else:\n            BGRimg = cv2.imread(uncorrected[i][0])\n        img = cv2.cvtColor(BGRimg, cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        \n    plt.show()\n")),(0,s.kt)("p",null,"The recommender has two outputs, an array of corrected results and an array of uncorrected results. Both outputs are important for analyzing the accuracy of the CNN model's feature extraction."),(0,s.kt)("p",null,"The corrected results removes images from the recommendation if they are found on the same website page. This corrected output is meant to recommend other artworks that reside on other pages to encourage the user to branch out to those pages.  "),(0,s.kt)("p",null,"The following is an example of what this recommendation looks like."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"recommend, uncorrected = vgg16Recommender(imgPaths[123], cosine_sim_tf,\n                                          k=10, showSim=True, sigFigs=4)\nrecommend\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('../Calla-Lily/public/images/5/yours-truly-you-2-allison-gable.png', 0.7143),\n ('../Calla-Lily/public/images/3/MoAM-kristy-lee-2.png', 0.7014),\n ('../Calla-Lily/public/images/2/may-flowers-2.png', 0.6026),\n ('../Calla-Lily/public/images/2023/to-build-castles-in-the-sky-spencer-vossman.webp',\n  0.5906),\n ('../Calla-Lily/public/images/2020/easy-tea-for-sore-throat.png', 0.5094),\n ('../Calla-Lily/public/images/5/porcelain-in-silks-guyon-perez.jpg', 0.4852),\n ('../Calla-Lily/public/images/4/dahlias-kristy-lee.jpg', 0.4719),\n ('../Calla-Lily/public/images/2020/in-honor-of-parasite-hemmy-chun.jpg',\n  0.4529),\n ('../Calla-Lily/public/images/2022/west-coast-elegies-amy-stukenholtz.jpg',\n  0.4491),\n ('../Calla-Lily/public/images/1/golden-boy-kelly-tran.png', 0.4449)]\n")),(0,s.kt)("p",null,"These similarity scores are fairly high when we consider that this is meant to model similarity over artworks that should not be remotely similar to each other. We can check the images to see how these numbers reflect what is shown."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"displayRecs(imgPaths[123], cosine_sim_tf)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"original img\n")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(5548).Z,width:"508",height:"418"})),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"recommendations (corrected)\n")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(8010).Z,width:"419",height:"418"})),(0,s.kt)("p",null,"There is good variation between the images recommended. We can see that this line art illustration does recommend illustrations with sharp lines or high contrast edges. This is expected behavior as there are no other line art illustrations except for the ones found on the same page."),(0,s.kt)("p",null,"The corrected result is what will be served as the final output to the frontend application. On the other hand, the uncorrected result is the recommendation based on all images in the artwork dataset. This means that an image in one website page could be recommended to a different image in the same webpage. This is good information for determining if the model is making accurate predictions by seeing if illustrations created by the same artist will recommend themselves for the same creative context. "),(0,s.kt)("p",null,'For example, we could test the model\'s outputs over the story "The Twelve Zodiac Animals". Helen Huang created many illustrations that have similar styles and thus they should result in a high similarity score compared to other illustrations in other stories or works.'),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"displayRecs(imgPaths[123], cosine_sim_tf, corrected=False)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"original img\n")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(3952).Z,width:"508",height:"418"})),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"recommendations (uncorrected)\n")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(2229).Z,width:"431",height:"418"})),(0,s.kt)("h3",{id:"fit-the-image-recommender-to-predict-posts"},"Fit the Image Recommender to Predict Posts"),(0,s.kt)("p",null,"The visual similarity model now works as expected to recommend other artworks. Now we would like to use this recommender to predict different pages with respect to the image similarities. To do this we need to make the image similarity's cosine matrix match the same shape as the number of posts. We can reformat our CNN's feature predictions by the webpage instead of by the image."),(0,s.kt)("p",null,"For pages without images, there won't be any features and thus the weights will all be zero. "),(0,s.kt)("p",null,"One particular issue is how to combine the features in a page that has more than one artwork. We could take the mean vector to come up with an angle in the center of the image differences. This is the better approach than taking the max/min because that will cause the same problems that Euclidean distances has. The average angle would represent how the post's image styles are represented together. And usually the posts with complementary illustrations are not too different from each other."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"clusterFeatsByPost = []\ntf_imgFeats = [(s,f) for s,f in zip(tf_imgSlugs, features_tf)]\n\nfor slug in postSlugs:\n    postFeats = []\n    imagesInPost = postImgs[slug]\n\n    # Posts without images\n    if len(imagesInPost) == 0:\n        postFeats = [0] * features_tf.shape[1]\n        clusterFeatsByPost.append(postFeats)\n        continue\n        \n    # Get the array of images in a post\n    for s, feat in tf_imgFeats:\n        if s in imagesInPost:\n            postFeats.append(feat)\n            \n    # Again, posts without images (because of GIFs)\n    if len(postFeats) == 0:\n        postFeats = [0] * features_tf.shape[1]\n        clusterFeatsByPost.append(postFeats)\n        continue\n\n    # Combine the features together\n    postFeats = np.mean(np.array(postFeats), axis=0)\n    clusterFeatsByPost.append(postFeats)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Expect: (# of posts, CNN output size)\nclusterFeatsByPost = np.array(clusterFeatsByPost)\nclusterFeatsByPost.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 4096)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_art = cosine_similarity(clusterFeatsByPost)\ncosine_sim_art.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 148)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Recommendation model with k closest works\ndef artRecommender(slug, cosine_sim, k=10, showSim=False, sigFigs=4):\n    idx = listRes.index(slug)\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:k+1]\n    if showSim:\n        sim_slugs = [(listRes[item[0]], round(item[1], sigFigs)) for item in sim_scores] \n    else:\n        sim_slugs = [listRes[item[0]] for item in sim_scores]\n    return sim_slugs\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"def printImgGrid(postSlug):\n    fig = plt.figure()\n    for i, imgPath in enumerate(postImgs[postSlug], start=1):\n        BGRimg = cv2.imread(imgPath)\n        img = cv2.cvtColor(BGRimg, cv2.COLOR_BGR2RGB)\n\n        ax = fig.add_subplot(1, len(postImgs[postSlug]), i)\n        ax.get_yaxis().set_visible(False)\n        ax.get_xaxis().set_visible(False)\n        ax.imshow(img)\n    plt.show()\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"def displayRecImgs(slug, rec_outputs):\n    printImgGrid(slug)\n        \n    for postSlug, _ in rec_outputs:\n        printImgGrid(postSlug)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"test_input = '/2/may-flowers'\nk = 10\nshowSim = True\nsigFigs=4\nrecommend = artRecommender(test_input, cosine_sim_art,\n                               k=10, showSim=True, sigFigs=4) \nrecommend\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"[('/5/the-twelve-zodiac-animals-visit', 0.6815),\n ('/1/the-dual-edged-life-of-mary-read', 0.6342),\n ('/3/the-middle-of-all-middles', 0.629),\n ('/1/at-the-sink', 0.5701),\n ('/2020/parasite', 0.5691),\n ('/2023/to-build-castles-in-the-sky', 0.5688),\n ('/2020/rain-on-the-rooftops', 0.5454),\n ('/5/you-grew-on-me', 0.5158),\n ('/5/to-you-a-reply', 0.5049),\n ('/1/golden-boy', 0.493)]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"displayRecImgs(test_input, recommend)\n")),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(3895).Z,width:"515",height:"314"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(527).Z,width:"516",height:"172"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(227).Z,width:"515",height:"236"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(3185).Z,width:"516",height:"181"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(6461).Z,width:"303",height:"389"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(7174).Z,width:"297",height:"389"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(8654).Z,width:"305",height:"389"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(1125).Z,width:"516",height:"186"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(6768).Z,width:"515",height:"388"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(7846).Z,width:"513",height:"389"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"png",src:n(2375).Z,width:"204",height:"389"})),(0,s.kt)("h2",{id:"combining-the-artwork-model-with-literature-model"},"Combining the Artwork Model with Literature Model"),(0,s.kt)("p",null,"Now we would like to figure out how to combine the visual arts recommendation with the literary recommendations. At this point, we have already defined recommendation models for literature and visual arts independent of each other. We have also built cosine matrices that have the same shape as the number of posts on the website."),(0,s.kt)("p",null,'One approach is to keep the literature and visual models separate from each other and run the visual arts recommendations on pages tagged with "Visual Arts". This has the benefit of not integrating the two systems together which may cause inaccurate predictions over everything if done improperly. However, this approach fails to diversify the user\'s recommendations as this will lead to visual arts pages recommended to other visual arts pages or literature while prose (fiction, nonfiction, poetry) pages will continue to be recommended to other prose pages with little to no consideration for visual arts pages.'),(0,s.kt)("hr",null),(0,s.kt)("p",null,"A better approach utilizes what is similar about the content-based recommender and visual similarity recommender: ",(0,s.kt)("strong",{parentName:"p"},"the cosine similarity"),". We have proved this is possible before in the content-based recommender by combining the metadata feature vector with the content TF-IDF feature vector. The images had their features extracted to a 4096 vector space, so we can extend the content-based feature vector with this. "),(0,s.kt)("p",null,"This should theoretically allow any literary arts page to recommend other literary arts page with nonzero similarities between each of them and thus give each post a chance to recommend another. Therefore, visual arts works could be recommended on prose pages that already have images. It is expected that this situation has a low chance of occurring since the TF-IDF vector probably has a strong affinity for other works with text instead of a webpage with no text.  "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Test Suite for expected recommendations based on obvious relevancies\ndef test_system(cosine_sim):\n    test_A = recommender_system('/3/blame-and-balm', cosine_sim,\n                                   k=10, showSim=False, sigFigs=5) \n    test_B = recommender_system('/2/when-the-partys-over-interview', cosine_sim,\n                                   k=10, showSim=False, sigFigs=5)\n    test_C = recommender_system('/5/cognates', cosine_sim,\n                                   k=10, showSim=False, sigFigs=5)\n    test_D = recommender_system('/2023/missed-connections', cosine_sim,\n                                   k=10, showSim=False, sigFigs=5)\n    \n    print('Test A\\n  /3/blame-and-balm | Expect: /3/blame-and-balm-interview')\n    print(test_A)\n    print('Success!' if '/3/blame-and-balm-interview' in test_A else 'FAILED') \n    \n    print('\\nTest B\\n  /2/when-the-partys-over-interview | Expect: /2/when-the-partys-over')\n    print(test_B)\n    print('Success!' if '/2/when-the-partys-over' in test_B else 'FAILED') \n    \n    print('\\nTest C\\n  /5/cognates | Expect: para-mi-hermane (Spanish)')\n    print(test_C)\n    print('Success!' if '/4/para-mi-hermane' in test_C else 'FAILED')\n    \n    print('\\nTest D\\n /2023/missed-connections | Expect: /6/you-have-created-an-imaginary-friend')\n    print(test_D)\n    print('Success!' if '/6/you-have-created-an-imaginary-friend' in test_D else 'FAILED')\n\n")),(0,s.kt)("h3",{id:"combination-1-visual-features-and-meta-tf-idf"},"Combination 1: Visual Features and (Meta-TF-IDF)"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"print(clusterFeatsByPost.shape)\nprint(meta_tfidf_matrix.shape)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 4096)\n(148, 103094)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# clusterFeatsByPost is a scipy sparse matrix, so\n# concatenating the feature vectors requires a scipy concatenation function\nfrom scipy.sparse import hstack\n\nlitart_matrix = hstack((clusterFeatsByPost, meta_tfidf_matrix))\n\ncosine_sim_litart_1 = cosine_similarity(litart_matrix, litart_matrix)\ncosine_sim_litart_1.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 148)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"recs_litart = {}\nfor slug in listRes:\n    recommendations = recommender_system(slug, cosine_sim_litart_1,\n                                         k=10, showSim=False, sigFigs=4)\n    recs_litart[slug] = recommendations\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"direct_recs_litart = test_rec_diversity(recs_litart, showConnections=False)\ndirect_counts_litart = {}\nfor k,v in direct_recs_litart:\n    direct_counts_litart[v] = direct_counts_litart.get(v, 0) + 1\nprint('(k, num) posts with direct recommendations:', direct_counts_litart)\ndirect_recs_litart[-10:]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(k, num) posts with direct recommendations: {0: 38, 1: 20, 2: 14, 3: 14, 4: 8, 5: 13, 6: 3, 7: 4, 8: 4, 9: 3, 10: 3, 11: 3, 12: 2, 13: 4, 15: 5, 16: 1, 18: 1, 20: 1, 22: 1, 30: 1, 32: 2, 34: 1, 35: 1, 38: 1}\n\n\n\n\n\n[('/2023/to-build-castles-in-the-sky', 16),\n ('/2023/overpriced', 18),\n ('/2023/always-the-victim', 20),\n ('/2023/a-sculpture-from-scratch', 22),\n ('/5/lament-for-the-old-man', 30),\n ('/2023/nesting-dolls-and-snakeskin', 32),\n ('/2023/outdated', 32),\n ('/3/the-middle-of-all-middles', 34),\n ('/5/the-twelve-zodiac-animals-visit', 35),\n ('/2023/homewrecker', 38)]\n")),(0,s.kt)("p",null,"There are less posts that do not get any direct recommendations in this combining method than the three recommendation systems we have built so far. However, we can do better than this. If you modify the above code block to show the recommendation graph's connected components, there are narrower groupings with the largest group being around 84 and the smallest around 35."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"test_system(cosine_sim_litart_1)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Test A\n  /3/blame-and-balm | Expect: /3/blame-and-balm-interview\n['/1/skin-vilar', '/2020/rain-on-the-rooftops', '/1/at-the-sink', '/3/the-middle-of-all-middles', '/1/the-dual-edged-life-of-mary-read', '/2/little-doll', '/2020/parasite', '/2/may-flowers', '/4/in-a-haze', '/1/untitled']\nFAILED\n\nTest B\n  /2/when-the-partys-over-interview | Expect: /2/when-the-partys-over\n['/2022/thats-so-valid', '/3/blame-and-balm-interview', '/2021/requiem', '/2022/validation', '/2023/homewrecker', '/5/lament-for-the-old-man', '/2023/a-sculpture-from-scratch', '/2023/outdated', '/2021/manifest', '/4/worm']\nFAILED\n\nTest C\n  /5/cognates | Expect: para-mi-hermane (Spanish)\n['/3/the-middle-of-all-middles', '/5/the-twelve-zodiac-animals-visit', '/5/to-you-a-reply', '/5/you-grew-on-me', '/2/little-doll', '/2/underdog', '/2020/rain-on-the-rooftops', '/4/retreat', '/4/in-a-haze', '/2022/west-coast-elegies']\nFAILED\n\nTest D\n /2023/missed-connections | Expect: /6/you-have-created-an-imaginary-friend\n['/2023/homewrecker', '/2022/thats-so-valid', '/2023/soliloquy', '/2022/tags-vaccine-vignettes-slice-of-life', '/2021/manifest', '/2021/requiem', '/2023/outdated', '/4/worm', '/2023/overpriced', '/2023/a-sculpture-from-scratch']\nFAILED\n")),(0,s.kt)("p",null,"All of the tests failed, so there is something wrong with this method. If we look closely and compare the literary features to the visual features, we will notice a difference in number ranges."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"print(np.min(clusterFeatsByPost), np.max(clusterFeatsByPost), np.mean(clusterFeatsByPost))\nprint(np.min(tfidf_matrix), np.max(tfidf_matrix), np.mean(tfidf_matrix))\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"0.0 16.24488067626953 0.3267293222098781\n0.0 0.7255359357857215 0.0002831744287036499\n")),(0,s.kt)("h3",{id:"combination-2-normalized-visual-features-and-meta-tf-idf"},"Combination 2: Normalized Visual Features and (Meta-TF-IDF)"),(0,s.kt)("p",null,"It is possible that the data is being skewed significantly because of the difference in not only magnitudes but also averages. Let's try to normalize these extracted features."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.preprocessing import normalize\nnorm_visual = normalize(clusterFeatsByPost)\nnorm_tfidf = normalize(tfidf_matrix)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"litart_matrix_2 = hstack((norm_visual, norm_tfidf))\n\ncosine_sim_litart_2 = cosine_similarity(litart_matrix_2, litart_matrix_2)\ncosine_sim_litart_2.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 148)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"recs_litart_2 = {}\nfor slug in listRes:\n    recommendations = recommender_system(slug, cosine_sim_litart_2,\n                                         k=10, showSim=False, sigFigs=4)\n    recs_litart_2[slug] = recommendations\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"direct_recs_litart_2 = test_rec_diversity(recs_litart_2, showConnections=False)\ndirect_counts_litart_2 = {}\nfor k,v in direct_recs_litart_2:\n    direct_counts_litart_2[v] = direct_counts_litart_2.get(v, 0) + 1\nprint('(k, num) posts with direct recommendations:', direct_counts_litart_2)\ndirect_recs_litart_2[-10:]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(k, num) posts with direct recommendations: {0: 50, 1: 24, 2: 11, 3: 12, 4: 5, 5: 6, 6: 4, 7: 2, 8: 1, 9: 3, 10: 4, 11: 3, 12: 5, 14: 1, 15: 1, 16: 1, 17: 2, 18: 3, 21: 1, 23: 2, 25: 1, 27: 1, 28: 2, 31: 1, 36: 1, 37: 1}\n\n\n\n\n\n[('/5/the-twelve-zodiac-animals-visit', 21),\n ('/5/lament-for-the-old-man', 23),\n ('/3/the-middle-of-all-middles', 23),\n ('/4/love-blooms', 25),\n ('/2023/outdated', 27),\n ('/2023/nesting-dolls-and-snakeskin', 28),\n ('/5/porcelain-in-silks', 28),\n ('/5/invitation', 31),\n ('/4/in-a-haze', 36),\n ('/2023/homewrecker', 37)]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"test_system(cosine_sim_litart_2)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Test A\n  /3/blame-and-balm | Expect: /3/blame-and-balm-interview\n['/1/the-dual-edged-life-of-mary-read', '/4/in-a-haze', '/1/untitled', '/1/treasure', '/1/skin-vilar', '/2020/rain-on-the-rooftops', '/5/porcelain-in-silks', '/4/eclectic-romantic', '/3/the-middle-of-all-middles', '/1/at-the-sink']\nFAILED\n\nTest B\n  /2/when-the-partys-over-interview | Expect: /2/when-the-partys-over\n['/2022/thats-so-valid', '/2021/requiem', '/3/blame-and-balm-interview', '/2/when-the-partys-over', '/2022/validation', '/2023/homewrecker', '/5/lament-for-the-old-man', '/2021/manifest', '/2023/outdated', '/2023/a-sculpture-from-scratch']\nSuccess!\n\nTest C\n  /5/cognates | Expect: para-mi-hermane (Spanish)\n['/2/underdog', '/4/in-a-haze', '/1/dissonance', '/3/the-divine-diablo-dichotomy', '/4/eclectic-romantic', '/1/the-dual-edged-life-of-mary-read', '/4/wet-t-issues', '/3/the-middle-of-all-middles', '/5/the-temporariness-of-twenty', '/5/the-twelve-zodiac-animals-visit']\nFAILED\n\nTest D\n /2023/missed-connections | Expect: /6/you-have-created-an-imaginary-friend\n['/2023/homewrecker', '/2022/thats-so-valid', '/2022/tags-vaccine-vignettes-slice-of-life', '/2023/soliloquy', '/2021/requiem', '/2021/manifest', '/4/mirror-shards', '/2023/overpriced', '/2023/outdated', '/4/the-disappearing-act']\nFAILED\n")),(0,s.kt)("h3",{id:"combination-3-mean-cosine-visual-and-meta-tf-idf"},"Combination 3: Mean Cosine Visual and (Meta-TF-IDF)"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_litart_3 = np.mean(np.array([cosine_sim_meta_tfidf, cosine_sim_art]), axis=0)\ncosine_sim_litart_3\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"array([[0.5       , 0.02051022, 0.00903303, ..., 0.00889781, 0.00864913,\n        0.00413398],\n       [0.02051022, 0.5       , 0.00547128, ..., 0.00375249, 0.00672675,\n        0.00221688],\n       [0.00903303, 0.00547128, 0.5       , ..., 0.00490627, 0.00105102,\n        0.00181298],\n       ...,\n       [0.00889781, 0.00375249, 0.00490627, ..., 1.        , 0.14051752,\n        0.22991886],\n       [0.00864913, 0.00672675, 0.00105102, ..., 0.14051752, 1.        ,\n        0.14537836],\n       [0.00413398, 0.00221688, 0.00181298, ..., 0.22991886, 0.14537836,\n        1.        ]])\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"recs_litart_3 = {}\nfor slug in listRes:\n    recommendations = recommender_system(slug, cosine_sim_litart_3,\n                                         k=10, showSim=False, sigFigs=4)\n    recs_litart_3[slug] = recommendations\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"direct_recs_litart_3 = test_rec_diversity(recs_litart_3, showConnections=False)\ndirect_counts_litart_3 = {}\nfor k,v in direct_recs_litart_3:\n    direct_counts_litart_3[v] = direct_counts_litart_3.get(v, 0) + 1\nprint('(k, num) posts with direct recommendations:', direct_counts_litart_3)\ndirect_recs_litart_3[-10:]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(k, num) posts with direct recommendations: {0: 39, 1: 24, 2: 8, 3: 12, 4: 7, 5: 11, 6: 9, 7: 5, 8: 5, 9: 4, 10: 8, 11: 2, 13: 1, 14: 1, 16: 1, 17: 1, 18: 4, 21: 2, 22: 1, 28: 1, 29: 1, 33: 1}\n\n\n\n\n\n[('/2023/nesting-dolls-and-snakeskin', 18),\n ('/2/may-flowers', 18),\n ('/2023/outdated', 18),\n ('/4/in-a-haze', 18),\n ('/4/mirror-shards', 21),\n ('/3/sing', 21),\n ('/5/lament-for-the-old-man', 22),\n ('/5/the-twelve-zodiac-animals-visit', 28),\n ('/3/the-middle-of-all-middles', 29),\n ('/2023/homewrecker', 33)]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"test_system(cosine_sim_litart_3)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Test A\n  /3/blame-and-balm | Expect: /3/blame-and-balm-interview\n['/1/skin-vilar', '/2020/rain-on-the-rooftops', '/1/at-the-sink', '/3/the-middle-of-all-middles', '/2/little-doll', '/1/the-dual-edged-life-of-mary-read', '/2/may-flowers', '/2020/parasite', '/4/in-a-haze', '/5/the-twelve-zodiac-animals-visit']\nFAILED\n\nTest B\n  /2/when-the-partys-over-interview | Expect: /2/when-the-partys-over\n['/2/when-the-partys-over', '/2022/thats-so-valid', '/3/blame-and-balm-interview', '/2021/requiem', '/2022/validation', '/2023/homewrecker', '/5/lament-for-the-old-man', '/2021/sand-is-not-good-for-sunflowers', '/2023/a-sculpture-from-scratch', '/3/the-middle-of-all-middles']\nSuccess!\n\nTest C\n  /5/cognates | Expect: para-mi-hermane (Spanish)\n['/3/the-middle-of-all-middles', '/5/the-twelve-zodiac-animals-visit', '/5/to-you-a-reply', '/5/you-grew-on-me', '/2020/rain-on-the-rooftops', '/2/little-doll', '/4/retreat', '/2/underdog', '/4/in-a-haze', '/2022/west-coast-elegies']\nFAILED\n\nTest D\n /2023/missed-connections | Expect: /6/you-have-created-an-imaginary-friend\n['/4/mirror-shards', '/2023/homewrecker', '/2022/thats-so-valid', '/2023/soliloquy', '/2022/tags-vaccine-vignettes-slice-of-life', '/2021/manifest', '/2021/requiem', '/2023/outdated', '/4/worm', '/2023/overpriced']\nFAILED\n")),(0,s.kt)("h3",{id:"combination-4-mean-cosine-visual--tf-idf--metadata-count"},"Combination 4: Mean Cosine Visual + TF-IDF + Metadata Count"),(0,s.kt)("p",null,"Is there a better way to combine the recommendation results together? We have four different feature vectors and four different cosine similarity matrices. We found that the metadata counts had high base similarities on their own while TF-IDF on the content did not contribute to predictions to a impactful degree. Combining metadata with TF-IDF may have had an adverse effect on the final predictions. If the average of the cosine matrices were taken after isolating each part, it would place equal weights on the three similarity matrices. This may lead to visual art recommendations having relevance and interacting with literary works and vice versa."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_count.shape\ncosine_sim_tfidf.shape\ncosine_sim_meta_tfidf.shape\ncosine_sim_art.shape\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(148, 148)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"cosine_sim_3 = np.mean(np.array([cosine_sim_count, cosine_sim_tfidf, cosine_sim_art]), axis=0)\ncosine_sim_3\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"array([[0.66666667, 0.0196745 , 0.00938556, ..., 0.00896395, 0.00882039,\n        0.00446968],\n       [0.0196745 , 0.66666667, 0.07367548, ..., 0.06456939, 0.00694719,\n        0.06306029],\n       [0.00938556, 0.07367548, 0.66666667, ..., 0.0796782 , 0.00117573,\n        0.07614893],\n       ...,\n       [0.00896395, 0.06456939, 0.0796782 , ..., 1.        , 0.0945829 ,\n        0.22206902],\n       [0.00882039, 0.00694719, 0.00117573, ..., 0.0945829 , 1.        ,\n        0.09797347],\n       [0.00446968, 0.06306029, 0.07614893, ..., 0.22206902, 0.09797347,\n        1.        ]])\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"recs_3 = {}\nfor slug in listRes:\n    recommendations = recommender_system(slug, cosine_sim_3,\n                                         k=10, showSim=False, sigFigs=4)\n    recs_3[slug] = recommendations\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"direct_recs_3 = test_rec_diversity(recs_3, showConnections=False)\ndirect_counts_3 = {}\nfor k,v in direct_recs_3:\n    direct_counts_3[v] = direct_counts_3.get(v, 0) + 1\nprint('(k, num) posts with direct recommendations:', direct_counts_3)\ndirect_recs_3[-10:]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(k, num) posts with direct recommendations: {0: 27, 1: 21, 2: 12, 3: 21, 4: 14, 5: 6, 6: 8, 7: 6, 8: 7, 9: 6, 10: 4, 11: 1, 12: 1, 13: 2, 14: 1, 16: 2, 17: 3, 19: 3, 20: 1, 24: 2}\n\n\n\n\n\n[('/4/in-a-haze', 16),\n ('/2023/planet', 17),\n ('/2023/overpriced', 17),\n ('/2023/to-build-castles-in-the-sky', 17),\n ('/2023/homewrecker', 19),\n ('/2023/too-easily', 19),\n ('/2023/out-of-use', 19),\n ('/5/the-twelve-zodiac-animals-visit', 20),\n ('/2023/outdated', 24),\n ('/5/to-you-a-reply', 24)]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"test_system(cosine_sim_3)\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Test A\n  /3/blame-and-balm | Expect: /3/blame-and-balm-interview\n['/1/skin-vilar', '/1/at-the-sink', '/2/little-doll', '/2/fishbowl-brain', '/3/sing', '/5/cognates', '/2022/west-coast-elegies', '/2023/metamorphoses', '/3/blame-and-balm-interview', '/3/grandfather']\nSuccess!\n\nTest B\n  /2/when-the-partys-over-interview | Expect: /2/when-the-partys-over\n['/2/when-the-partys-over', '/2022/the-mortal-on-the-crosswalk', '/2/toast-interview', '/3/blame-and-balm-interview', '/3/mother-moon-interview', '/2022/thats-so-valid', '/2021/requiem', '/2/may-flowers', '/2020/daylily', '/1/with-liqour']\nSuccess!\n\nTest C\n  /5/cognates | Expect: para-mi-hermane (Spanish)\n['/5/to-you-a-reply', '/5/you-grew-on-me', '/1/at-the-sink', '/4/retreat', '/2/little-doll', '/5/yours-truly-you', '/3/say-grace', '/2022/west-coast-elegies', '/5/underneath-the-magnolia', '/2023/to-build-castles-in-the-sky']\nFAILED\n\nTest D\n /2023/missed-connections | Expect: /6/you-have-created-an-imaginary-friend\n['/2023/soliloquy', '/2023/hole', '/2023/poets-sleep-in-graves', '/2023/the-post-mortem', '/6/you-have-created-an-imaginary-friend', '/2023/too-easily', '/5/to-you-a-reply', '/2022/dictionary', '/2022/routines', '/2/cars']\nSuccess!\n")),(0,s.kt)("p",null,"Reviewing these results, less than 20% of all posts do not have a direct recommendation. This is much better than the 33% from any of the other approaches and cosine models. The outstanding tests mostly pass with the exception of Cognates, although this could be human error because one of them is not tagged with their translated language and thus it isn't going to show up as a strong features in the cosine similarity."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Convert the result to JSON and write to file\nwith open('recommender-3.json', mode='w', encoding='utf8') as outfile:\n    outfile.write(json.dumps(recs_3))\n")),(0,s.kt)("h2",{id:"closing-remarks"},"Closing Remarks"),(0,s.kt)("p",null,"This Notebook has explored the different approaches to recommending literary arts between other works irrespective of user preferences. Literature recommendations rely heavily on content-based similarity models such as weighing the frequency of terms found across all works as well as semantics to figure out how to best fit the content. The metadata also plays a large role here as creatives often make works that are similar in style to each other and thus their other works are usually ranked higher than other works. Our findings have shown that with a small dataset of texts to work with, there are very low similarity values when using the TF-IDF. It would be worth exploring CNNs applied to literature to determine if a more complex, learned approach would fit the predictions better."),(0,s.kt)("p",null,"Recommending art did require the use of CNNs to extract features from the images. Using a pretrained model over a million images helped to make art similarity have a higher accuracy as well as demonstrating that pretrained models can be modified to pull out features rather than making predictions for image classification. Ample testing and observations supplied positive feedback for the predictions made and thus made the visual similarity model work despite the subjectivity of art."),(0,s.kt)("p",null,"Combining models together was a process that didn't have clear supporting articles on how to do it properly. However, creating tests and experimenting with the similarity function led to success by inferring from the math and other operations it took to fundamentally recommendation systems. The recommender system for Other People Magazine is a representation of how multiple models can ultimately make profound recommendations that wasn't expected to be so accurate.  "),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"This notebook has an impact on the Other People Magazine by demonstrating that it is possible to perform data science operations over the organization's resources. The custom-built API can always be modified to optimize the kind of information needed to perform a task and thus serves as a use case for further exploration on what can be computationally done for the arts and humanities.")))}p.isMDXComponent=!0},5548:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_73_1-278992e3f1bfe97e0eca4e20a9a29c5a.png"},8010:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_73_3-3e435b068bba43b07aae697bebcfd873.png"},3952:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_76_1-278992e3f1bfe97e0eca4e20a9a29c5a.png"},2229:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_76_3-eca88a057e0e419968363b16c5867e8a.png"},3895:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_0-3900825ea59df8ad379e3c3aa5a6b67d.png"},527:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_1-a1058a2b462698d009a48903357facac.png"},2375:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_10-88369f3b0659a59f1f0874f60168d5df.png"},227:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_2-e05cd37ae5539de8353c20b62a70a025.png"},3185:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_3-2c021a6e0f11e0f75112e9ad7187705d.png"},6461:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_4-2bac7fd7971ad57eb1a34b2bce41a0fc.png"},7174:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_5-2ea1c8ee4eb666e2968b1d0143eb0052.png"},8654:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_6-f848a7ccb4c1ba0355f13904045e38cc.png"},1125:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_7-6c1808ed13e5283068e472c8b684915e.png"},6768:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_8-54e2fa07dc0ef1dffb732825232052cd.png"},7846:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/output_85_9-1c0667e0f75602b9ec7ceae94ef7717e.png"}}]);